{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For our project, we chose to use a data file from an adoption shelter in Austin, TX. The animals from the shelter were saved into a .csv file that we had to clean and then classify using a few different methods. We wanted to classify how long an incoming dog would spend at the shelter based on their breed, age, and a few other attributes. Being able to classify well how long an animal would stay at the shelter would be usful for the people running the shelters so the can work hard to get all the animals adopted and plan for their stay making sure they have everything they need to make the animals time in the shelter easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "To clean the data, we first had to remove all of the instances that were not dogs so cats, birds, etc. We then saved this data in a new file, dogs_data.csv. In order to classify the dogs and predict the time they would spend at the shelter, we had to remove quite a few attributes that were repeated or challenging to use. We kept most of the discretized and categorical data but removed the location they were found, their names, time they were taken in, time they were adopted, and any attributes that were repetative. We saved this file as clean_data.csv and it is the file we referred back to in order to get the data needed for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    attr, table = utils.parse_csv(\"adoption_data.csv\")\n",
    "\n",
    "    # Preserve animal entries for dogs and classifying attribute entry \n",
    "    animal_index = attr.index('animal_type_intake')\n",
    "    class_index = attr.index('time_bucket')\n",
    "    table = [row for row in table if row[animal_index] == 'Dog' and row[class_index] != '']\n",
    "\n",
    "    # Remove all duplicate entries \n",
    "    animal_ids = set()\n",
    "    animal_id_index = attr.index('animal_id')\n",
    "    for row in table:\n",
    "        # Check for duplicates\n",
    "        if row[animal_id_index] in animal_ids:\n",
    "            table.remove(row)\n",
    "        else: \n",
    "            print(row[animal_id_index])\n",
    "            animal_ids.add(row[animal_id_index]) \n",
    "    dogs_data = copy.deepcopy(table)\n",
    "    utils.write_csv('dogs_data.csv', attr, dogs_data)\n",
    "\n",
    "    # Remove attributes not to be trained on from instances in the dataset \n",
    "    remove_attr = ['animal_id', 'name_intake', 'date_time_intake', 'found_location', 'intake_condition', \n",
    "                    'animal_type_intake', 'month_year_intake', 'intake_sex', 'breed_intake', 'color_intake', \n",
    "                    'name_outcome', 'date_time_outcome', 'month_year_outcome','outcome_subtype', 'outcome_sex', \n",
    "                    'outcome_age', 'gender_outcome', 'fixed_intake', 'fixed_changed', 'date_time_length']\n",
    "\n",
    "    # Remove each attribute from all rows \n",
    "    for col in remove_attr: \n",
    "        index = attr.index(col)\n",
    "        attr.pop(index)\n",
    "        for row in table: \n",
    "            row.pop(index)    \n",
    "\n",
    "    utils.write_csv('clean_data.csv', attr, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(table, attr, attr_indexes, class_index): \n",
    "    '''\n",
    "    '''  \n",
    "    # Stratify data across 10 folds\n",
    "    stratified_data = utils.stratify_data(table, class_index, 10)\n",
    "\n",
    "    tp_tn = 0\n",
    "    for fold in stratified_data:\n",
    "        train_set = []\n",
    "        test_set = stratified_data.pop(fold)\n",
    "        for i in stratified_data:\n",
    "            train_set.extend(i)\n",
    "        \n",
    "        # Calculate probabilities of training set \n",
    "        classes, conditions, priors, posts = utils.prior_post_probabilities(train_set, attr, class_index, attr_indexes)\n",
    "\n",
    "        # Iterate through test set \n",
    "        for inst in test_set:\n",
    "            # Classify predicted and actual classes\n",
    "            pred_class = utils.naive_bayes(train_set, classes, conditions, attr, priors, posts, inst, class_index)\n",
    "            actual_class = inst[class_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "Using the decision tree classifer seemed like an easy step to take and would make it easy to create an ensemble classifier, but proved to be quite difficult. With the amount of data in the set, the trees were growing so large that it would take forever to classify an instance using all of the attributes to split on. Instead I took a bootstrapping approach to build one decision tree and set the max depth of the tree to be four attributes so the tree would be more useful. It was a trade off between accuracy and computational costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_classifier(table, original_table, attr_indexes, attr_domains, class_index, header, instance_to_classify):\n",
    "    '''\n",
    "    Calls the functions to get a decision tree for the data and uses that decision\n",
    "    tree and classifies a given instance. Returns the classification to main()\n",
    "    '''\n",
    "    rand_index = random.randint(0, len(table) - 1)\n",
    "    instance = table[rand_index]\n",
    "    print(\"Classifying instance: \", instance)\n",
    "    tree = tdidt(table, attr_indexes, attr_indexes, attr_domains, class_index, header, [])\n",
    "    utils.pretty_print(tree)\n",
    "    classification = decision_tree.classify_instance(header, instance, tree)\n",
    "    print(original_table[rand_index])\n",
    "    print(\"Classification: \", classification)\n",
    "\n",
    "    class_values = utils.get_attr_domains(table, header, [len(header) - 1])\n",
    "    print(class_values, class_index)\n",
    "    decision_tree.forest_classifier(table, attr_indexes, attr_domains, class_index, header, class_values, 10, 5)\n",
    "    \n",
    "\n",
    "def tdidt(instances, att_indexes, all_att_indexes, att_domains, class_index, header, tree):\n",
    "    '''\n",
    "    Uses the tdidt algorithm to build a decision tree based on a given set of data\n",
    "    '''\n",
    "    #print(\"Current Tree: \", tree)\n",
    "    #print(\"att_indexes = \", att_indexes)\n",
    "    if att_indexes == []:\n",
    "        return\n",
    "    att_index = entropy(instances, header, att_domains, att_indexes)\n",
    "    att_indexes.remove(att_index)\n",
    "    partition = partition_instances(instances, att_index, att_domains[header[att_index]])\n",
    "    partition_keys = partition.keys()\n",
    "    \n",
    "    tree.append(\"Attribute\")\n",
    "    tree.append(header[att_index])\n",
    "    count = 0\n",
    "    for i in range(len(att_domains[header[att_index]])):\n",
    "        #print(i)\n",
    "        tree.append([\"Value\", att_domains[header[att_index]][count]])\n",
    "        col = utils.get_column(partition.get(att_domains[header[att_index]][i]), len(header)-1)\n",
    "        items_in_col = []\n",
    "        for item in col:\n",
    "            if item not in items_in_col:\n",
    "                items_in_col.append(item)\n",
    "        if len(items_in_col) == 1:\n",
    "            tree[2+count].append([\"Leaves\", has_same_class_label(instances, header, att_index, class_index, col, items_in_col[0])])\n",
    "        elif len(att_indexes) == 0 and len(col) > 0:\n",
    "            majority_class = compute_partition_stats(col)\n",
    "            tree[2+count].append([\"Leaves\", has_same_class_label(instances, header, att_index, class_index, col, majority_class)])\n",
    "        elif col == []:\n",
    "            del tree[2+count]\n",
    "            return []\n",
    "        else:\n",
    "            tree[2+count].append([])\n",
    "            new_branch = [tdidt(partition.get(att_domains[header[att_index]][i]), att_indexes, all_att_indexes, att_domains, class_index, header, tree[2+count][2])]\n",
    "            if new_branch == [[]]:                \n",
    "                majority_class = compute_partition_stats(col)\n",
    "                tree[2][2] = [\"Leaves\", has_same_class_label(instances, header, att_index, class_index, col, majority_class)]\n",
    "            else:\n",
    "                tree[2][2] = new_branch\n",
    "        count += 1\n",
    "        att_indexes.append(att_index)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifier\n",
    "### Random Forest Method\n",
    "\n",
    "For our ensemble classifer, we used the random forest approach and built off of the decision tree classifier. In the bootstrapping method, we chose to split the trees on 4 attributes and select 10 of the best 20 trees created in order to keep computational costs at a minumum since it takes some time to go through all of that data, build multiple trees, test the trees on the validation set, and then use the forest to classify the instances in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "After working with the different classifiers it is easy to see why data mining is a tedious task with such large data sets. Working the thousands of instances made it hard to keep a high accuracy. Also with the data we chose we have concluded that it is hard to predict something that is very case by case. When people are adopting a dog it comes down to their preference in breed and gender, but also what dog they bond most with. It also depends on the dogs that are at the shelter the time they chose to adopt. There are so many uncontrolled variables in data such as this making is nearly impossible to get super accurate classifiers. However, through using our multiple classifiers we were able to come up with solutions that will classify how long a dog will spend at the shelter accurately enough that is could help the people that run the shelter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
